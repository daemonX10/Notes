# NLP Fundamentals (Tokenization, NER, POS Tagging) - Theory Questions

## Core Questions

## Question 1
**What is tokenization and why is it the fundamental first step in NLP pipelines?**
**Answer:** _To be filled_

---

## Question 2
**Explain the difference between stemming and lemmatization with examples.**
**Answer:** _To be filled_

---

## Question 3
**What are BPE, WordPiece, and SentencePiece? How do subword tokenization algorithms work?**
**Answer:** _To be filled_

---

## Question 4
**What is Named Entity Recognition (NER) and what are the common entity types?**
**Answer:** _To be filled_

---

## Question 5
**Explain the BIO/IOB tagging scheme used in NER and sequence labeling tasks.**
**Answer:** _To be filled_

---

## Question 6
**What is Part-of-Speech (POS) tagging? Why is it important for downstream NLP tasks?**
**Answer:** _To be filled_

---

## Question 7
**How do CRF (Conditional Random Fields) work for sequence labeling tasks like NER and POS tagging?**
**Answer:** _To be filled_

---

## Question 8
**What is the difference between rule-based, statistical, and neural approaches to NER?**
**Answer:** _To be filled_

---

## Question 9
**How do modern transformers like BERT handle tokenization differently from traditional methods?**
**Answer:** _To be filled_

---

## Question 10
**What is the out-of-vocabulary (OOV) problem and how do subword tokenizers solve it?**
**Answer:** _To be filled_

---

## Interview Questions

## Question 11
**How do you choose between word-level, subword, and character-level tokenization for different NLP tasks?**
**Answer:** _To be filled_

---

## Question 12
**What are the trade-offs between stemming and lemmatization for information retrieval?**
**Answer:** _To be filled_

---

## Question 13
**How do you handle tokenization for languages without clear word boundaries (Chinese, Japanese, Thai)?**
**Answer:** _To be filled_

---

## Question 14
**What techniques work best for tokenization of social media text with informal language, hashtags, and emojis?**
**Answer:** _To be filled_

---

## Question 15
**How do you handle NER for nested or overlapping entities?**
**Answer:** _To be filled_

---

## Question 16
**What strategies help with NER in low-resource languages with limited training data?**
**Answer:** _To be filled_

---

## Question 17
**How do you implement domain adaptation for NER models (e.g., from news to biomedical text)?**
**Answer:** _To be filled_

---

## Question 18
**What approaches work best for NER in noisy text like social media or OCR output?**
**Answer:** _To be filled_

---

## Question 19
**How do you handle entity disambiguation and linking NER results to knowledge bases?**
**Answer:** _To be filled_

---

## Question 20
**What are the challenges of POS tagging for morphologically rich languages?**
**Answer:** _To be filled_

---

## Question 21
**How do you handle ambiguous words that can have multiple POS tags depending on context?**
**Answer:** _To be filled_

---

## Question 22
**What is the role of context window in POS tagging and how do neural models capture it?**
**Answer:** _To be filled_

---

## Question 23
**How do you evaluate NER systems? Explain precision, recall, F1, and entity-level vs token-level metrics.**
**Answer:** _To be filled_

---

## Question 24
**What is few-shot NER and how can you identify new entity types with minimal examples?**
**Answer:** _To be filled_

---

## Question 25
**How do you handle tokenization for domain-specific texts like legal, medical, or code?**
**Answer:** _To be filled_

---

## Question 26
**What is the impact of tokenization choices on model vocabulary size and training efficiency?**
**Answer:** _To be filled_

---

## Question 27
**How do multilingual models like mBERT handle tokenization across different scripts and languages?**
**Answer:** _To be filled_

---

## Question 28
**What techniques help preserve named entities during tokenization (e.g., preventing "New York" from splitting)?**
**Answer:** _To be filled_

---

## Question 29
**How do you implement real-time NER for streaming text applications?**
**Answer:** _To be filled_

---

## Question 30
**What is cross-lingual transfer learning for NER? How does it work?**
**Answer:** _To be filled_

---

## Question 31
**How do you handle NER for entities that change over time (new celebrities, companies, products)?**
**Answer:** _To be filled_

---

## Question 32
**What are gazetteer features in NER and when should you use them?**
**Answer:** _To be filled_

---

## Question 33
**How do you balance precision vs recall in NER for different business use cases?**
**Answer:** _To be filled_

---

## Question 34
**What is the Universal Dependencies project and how does it standardize POS tagging across languages?**
**Answer:** _To be filled_

---

## Question 35
**How do you handle POS tagging for code-mixed or transliterated text?**
**Answer:** _To be filled_

---

## Question 36
**What role does dependency parsing play after POS tagging?**
**Answer:** _To be filled_

---

## Question 37
**How do transformer-based models perform joint NER and relation extraction?**
**Answer:** _To be filled_

---

## Question 38
**What is byte-level BPE and why do models like GPT use it?**
**Answer:** _To be filled_

---

## Question 39
**How do you debug tokenization issues affecting downstream model performance?**
**Answer:** _To be filled_

---

## Question 40
**What are the memory and latency considerations for different tokenization approaches in production?**
**Answer:** _To be filled_

---

## Question 41
**How do you handle NER confidence scoring and when to abstain from predictions?**
**Answer:** _To be filled_

---

## Question 42
**What is span-based NER and how does it differ from token classification approaches?**
**Answer:** _To be filled_

---

## Question 43
**How do you implement active learning for efficient NER annotation?**
**Answer:** _To be filled_

---

## Question 44
**What preprocessing steps are essential before tokenization (normalization, cleaning)?**
**Answer:** _To be filled_

---

## Question 45
**How do you handle special tokens ([CLS], [SEP], [PAD]) in transformer tokenizers?**
**Answer:** _To be filled_

---

## Question 46
**What is the difference between greedy and optimal tokenization algorithms?**
**Answer:** _To be filled_

---

## Question 47
**How do you implement privacy-preserving NER for sensitive documents (PII detection)?**
**Answer:** _To be filled_

---

## Question 48
**What are the best practices for fine-tuning pre-trained models on custom NER datasets?**
**Answer:** _To be filled_

---

## Question 49
**How do you handle NER for hierarchical or multi-level entity types?**
**Answer:** _To be filled_

---

## Question 50
**What metrics and techniques help evaluate tokenization quality?**
**Answer:** _To be filled_

---
