{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb6f4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference count: 2\n",
      "Memory usage: 104 bytes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "\n",
    "# Memory allocation example\n",
    "def demonstrate_memory_management():\n",
    "    # Objects are allocated in private heap\n",
    "    my_list = [1, 2, 3, 4, 5]  # Allocated in heap\n",
    "    \n",
    "    # Check object reference count\n",
    "    ref_count = sys.getrefcount(my_list)\n",
    "    print(f\"Reference count: {ref_count}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_usage = sys.getsizeof(my_list)\n",
    "    print(f\"Memory usage: {memory_usage} bytes\")\n",
    "\n",
    "demonstrate_memory_management()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3eea208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference count: 2\n",
      "Memory usage: 104 bytes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "\n",
    "# Memory allocation example\n",
    "def demonstrate_memory_management():\n",
    "    # Objects are allocated in private heap\n",
    "    my_list = [1, 2, 3, 4,8]  # Allocated in heap\n",
    "    \n",
    "    # Check object reference count\n",
    "    ref_count = sys.getrefcount(my_list)\n",
    "    print(f\"Reference count: {ref_count}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_usage = sys.getsizeof(my_list)\n",
    "    print(f\"Memory usage: {memory_usage} bytes\")\n",
    "\n",
    "demonstrate_memory_management()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8cfa5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial ref count: 2\n",
      "After assignment: 3\n",
      "After deletion: 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def reference_counting_demo():\n",
    "    # Create object\n",
    "    data = [1, 2, 3]\n",
    "    print(f\"Initial ref count: {sys.getrefcount(data)}\")\n",
    "    \n",
    "    # Assign to another variable\n",
    "    data2 = data\n",
    "    print(f\"After assignment: {sys.getrefcount(data)}\")\n",
    "    \n",
    "    # Delete reference\n",
    "    del data2\n",
    "    print(f\"After deletion: {sys.getrefcount(data)}\")\n",
    "reference_counting_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6170dd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects before GC: 81478\n",
      "Objects collected: 10\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import weakref\n",
    "\n",
    "class MLModel:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.data = []\n",
    "\n",
    "def garbage_collection_demo():\n",
    "    # Create circular reference\n",
    "    model1 = MLModel(\"Model1\")\n",
    "    model2 = MLModel(\"Model2\")\n",
    "    model1.partner = model2\n",
    "    model2.partner = model1\n",
    "    \n",
    "    # Check garbage collection\n",
    "    print(f\"Objects before GC: {len(gc.get_objects())}\")\n",
    "    \n",
    "    # Force garbage collection\n",
    "    collected = gc.collect()\n",
    "    print(f\"Objects collected: {collected}\")\n",
    "    \n",
    "    # Monitor object lifecycle\n",
    "    def callback(ref):\n",
    "        print(\"Object was garbage collected\")\n",
    "    \n",
    "    weak_ref = weakref.ref(model1, callback)\n",
    "\n",
    "garbage_collection_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "663984bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting memory-profiler\n",
      "  Using cached memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\damod\\anaconda3\\lib\\site-packages (from memory-profiler) (5.9.0)\n",
      "Using cached memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: memory-profiler\n",
      "Successfully installed memory-profiler-0.61.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install memory-profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from memory_profiler import profile\n",
    "\n",
    "class MemoryEfficientMLWorkflow:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.model = None\n",
    "    \n",
    "    @profile\n",
    "    def load_and_process_data(self, filepath):\n",
    "        \"\"\"Memory-efficient data loading\"\"\"\n",
    "        # Use chunking for large datasets\n",
    "        chunk_size = 10000\n",
    "        chunks = []\n",
    "        \n",
    "        for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "            # Process chunk\n",
    "            processed_chunk = self.preprocess_chunk(chunk)\n",
    "            chunks.append(processed_chunk)\n",
    "        \n",
    "        # Combine chunks efficiently\n",
    "        self.data = pd.concat(chunks, ignore_index=True)\n",
    "        \n",
    "        # Clear intermediate variables\n",
    "        del chunks\n",
    "        gc.collect()\n",
    "    \n",
    "    def preprocess_chunk(self, chunk):\n",
    "        \"\"\"Memory-efficient preprocessing\"\"\"\n",
    "        # Use view instead of copy when possible\n",
    "        numeric_columns = chunk.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Optimize data types\n",
    "        chunk = self.optimize_dtypes(chunk)\n",
    "        \n",
    "        return chunk\n",
    "    \n",
    "    def optimize_dtypes(self, df):\n",
    "        \"\"\"Optimize pandas dtypes to reduce memory\"\"\"\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'int64':\n",
    "                if df[col].min() >= 0 and df[col].max() <= 255:\n",
    "                    df[col] = df[col].astype('uint8')\n",
    "                elif df[col].min() >= -128 and df[col].max() <= 127:\n",
    "                    df[col] = df[col].astype('int8')\n",
    "            elif df[col].dtype == 'float64':\n",
    "                df[col] = df[col].astype('float32')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup when object is destroyed\"\"\"\n",
    "        if hasattr(self, 'data') and self.data is not None:\n",
    "            del self.data\n",
    "        gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaeec93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class ModelRegistry:\n",
    "    \"\"\"Example of dictionary usage in ML workflows\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Store multiple models with configurations\n",
    "        self.models = {}\n",
    "        self.metrics = {}\n",
    "        self.feature_importance = {}\n",
    "    \n",
    "    def register_model(self, model_name, model, config):\n",
    "        \"\"\"Register model with its configuration\"\"\"\n",
    "        self.models[model_name] = {\n",
    "            'model_object': model,\n",
    "            'config': config,\n",
    "            'trained': False,\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def store_metrics(self, model_name, metrics_dict):\n",
    "        \"\"\"Store evaluation metrics for a model\"\"\"\n",
    "        self.metrics[model_name] = metrics_dict\n",
    "    \n",
    "    def get_best_model(self, metric='accuracy'):\n",
    "        \"\"\"Find best performing model based on metric\"\"\"\n",
    "        if not self.metrics:\n",
    "            return None\n",
    "        \n",
    "        best_model = max(\n",
    "            self.metrics.items(),\n",
    "            key=lambda x: x[1].get(metric, 0)\n",
    "        )\n",
    "        return best_model[0]  # Return model name\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare all models using stored metrics\"\"\"\n",
    "        comparison = {}\n",
    "        for model_name, metrics in self.metrics.items():\n",
    "            comparison[model_name] = {\n",
    "                'accuracy': metrics.get('accuracy', 0),\n",
    "                'precision': metrics.get('precision', 0),\n",
    "                'recall': metrics.get('recall', 0),\n",
    "                'f1_score': metrics.get('f1_score', 0)\n",
    "            }\n",
    "        return comparison\n",
    "\n",
    "# Usage example\n",
    "registry = ModelRegistry()\n",
    "\n",
    "# Register models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rf_config = {'n_estimators': 100, 'max_depth': 10}\n",
    "svm_config = {'C': 1.0, 'kernel': 'rbf'}\n",
    "\n",
    "registry.register_model('random_forest', RandomForestClassifier(**rf_config), rf_config)\n",
    "registry.register_model('svm', SVC(**svm_config), svm_config)\n",
    "\n",
    "# Store metrics\n",
    "registry.store_metrics('random_forest', {\n",
    "    'accuracy': 0.85, 'precision': 0.83, 'recall': 0.87, 'f1_score': 0.85\n",
    "})\n",
    "registry.store_metrics('svm', {\n",
    "    'accuracy': 0.82, 'precision': 0.80, 'recall': 0.84, 'f1_score': 0.82\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86763701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello world', 'python ml', 'data science']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = ['Hello World!', 'Python ML', 'Data Science@']\n",
    "cleaned_text = [\n",
    "    ''.join([char.lower() for char in text if char.isalnum() or char.isspace()])\n",
    "    for text in text_data\n",
    "]\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375ef91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m log_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m skewed_features]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skewed_features):\n\u001b[1;32m---> 23\u001b[0m     df[log_features[i]] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog1p(df[col])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Create polynomial features\n",
    "    numeric_cols = ['age', 'income', 'credit_score']\n",
    "    polynomial_features = [\n",
    "        f\"{col}_squared\" for col in numeric_cols\n",
    "    ]\n",
    "    \n",
    "    # Add polynomial features to dataframe\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        df[polynomial_features[i]] = df[col] ** 2\n",
    "    \n",
    "    # Create interaction features\n",
    "    interaction_features = [\n",
    "        f\"{col1}_{col2}_interaction\" \n",
    "        for i, col1 in enumerate(numeric_cols)\n",
    "        for col2 in numeric_cols[i+1:]\n",
    "    ]\n",
    "    \n",
    "    # Log transformations for skewed features\n",
    "    skewed_features = ['income', 'credit_score']\n",
    "    log_features = [f\"log_{col}\" for col in skewed_features]\n",
    "    \n",
    "    for i, col in enumerate(skewed_features):\n",
    "        df[log_features[i]] = np.log1p(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122eda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48e9a121",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
